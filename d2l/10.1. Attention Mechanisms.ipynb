{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('1.6.0', '2.3.1', '1.7.0')"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import torch as tc\n",
    "import tensorflow as tf\n",
    "import mxnet as mx\n",
    "from mxnet import np as mxnp\n",
    "from mxnet import npx as npx\n",
    "npx.set_np()\n",
    "# import numpy as np\n",
    "\n",
    "from d2l import mxnet as mxd2l  # Use MXNet as the backend\n",
    "from d2l import torch as tcd2l  # Use PyTorch as the backend\n",
    "from d2l import tensorflow as tfd2l  # Use TensorFlow as the backend\n",
    "\n",
    "tc.__version__,tf.__version__,mx.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mxnet.gluon import nn\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def masked_softmax(X,valid_len):\n",
    "    \"\"\"Perform softmax by filtering out some elements.\"\"\"\n",
    "    # X: 3-D tensor, valid_len: 1-D or 2-D tensor\n",
    "    if valid_len is None:\n",
    "        return npx.softmax(X)\n",
    "    else:\n",
    "        shape=X.shape\n",
    "        if valid_len.ndim==1:\n",
    "            valid_len=valid_len.repeat(shape[1],axis=0)\n",
    "        else:\n",
    "            valid_len=valid_len.reshape(-1)\n",
    "        # Fill masked elements with a large negative, whose exp is 0\n",
    "        X=npx.sequence_mask(X.reshape(-1,shape[-1]),valid_len,True,axis=1,value=-1e6)\n",
    "        return npx.softmax(X).reshape(shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1.]],\n",
       "\n",
       "       [[1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1.]]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# X=mxnp.random.uniform(size=(2, 2, 4))\n",
    "X=mxnp.ones((2, 2, 4))\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_len=mxnp.array([2,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0.5       , 0.5       , 0.        , 0.        ],\n",
       "        [0.5       , 0.5       , 0.        , 0.        ]],\n",
       "\n",
       "       [[0.33333334, 0.33333334, 0.33333334, 0.        ],\n",
       "        [0.33333334, 0.33333334, 0.33333334, 0.        ]]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "masked_softmax(X,valid_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_len=mxnp.array([[1,2],[3,4]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[1.        , 0.        , 0.        , 0.        ],\n",
       "        [0.5       , 0.5       , 0.        , 0.        ]],\n",
       "\n",
       "       [[0.33333334, 0.33333334, 0.33333334, 0.        ],\n",
       "        [0.25      , 0.25      , 0.25      , 0.25      ]]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "masked_softmax(X,valid_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DotProductAttention(nn.Block):\n",
    "    def __init__(self, dropout, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    # `query`: (`batch_size`, #queries,  `d`)\n",
    "    # `key`:   (`batch_size`, #kv_pairs, `d`)\n",
    "    # `value`: (`batch_size`, #kv_pairs, `dim_v`)\n",
    "    # `valid_len`: either (`batch_size`, ) or (`batch_size`, (`batch_size`* #queries))\n",
    "    def forward(self, query, key, value, valid_len=None):\n",
    "        d = query.shape[-1]\n",
    "        # Set transpose_b=True to swap the last two dimensions of key\n",
    "        scores = npx.batch_dot(query, key, transpose_b=True) / math.sqrt(d)\n",
    "        attention_weights = self.dropout(masked_softmax(scores, valid_len))\n",
    "        return npx.batch_dot(attention_weights, value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLPAttention(nn.Block):\n",
    "    def __init__(self,hidden_units,dropout,**kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        # Use flatten=False to keep query's and key's 3-D shapes\n",
    "        self.W_q=nn.Dense(hidden_units,use_bias=False,flatten=False)\n",
    "        self.W_k=nn.Dense(hidden_units,use_bias=False,flatten=False)\n",
    "        self.v=nn.Dense(1,use_bias=False,flatten=False)\n",
    "        self.dropout=nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self,query,key,value,valid_len):\n",
    "        query=self.W_q(query)\n",
    "        key  =self.W_k(key)\n",
    "        # Expand query to (`batch_size`, #queries, 1,  units),\n",
    "        #    and  key  to (`batch_size`, 1, #kv_pairs, units). Then plus them with broadcast\n",
    "        feature_q=mxnp.expand_dims(query,axis=2)\n",
    "        feature_k=mxnp.expand_dims(key,axis=1)\n",
    "        features=feature_q+feature_k\n",
    "        features=mxnp.tanh(features)\n",
    "        scores=self.v(features)\n",
    "        scores=mxnp.squeeze(scores,axis=-1)\n",
    "        attention_weights=self.dropout(masked_softmax(scores, valid_len))\n",
    "        return npx.batch_dot(attention_weights, value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DotProductAttention_unittest():\n",
    "    atten = DotProductAttention(dropout=0.5)\n",
    "    atten.initialize()\n",
    "\n",
    "    query=mxnp.ones((2, 2, 2))   #(2, 2,  2)\n",
    "    key=  mxnp.ones((2, 10, 2))  #(2, 10, 2)\n",
    "    #                       value (2, 10, 4)\n",
    "    value= mxnp.arange(40).reshape(1, 10, 4).repeat(2, axis=0)\n",
    "    valid_len1 = mxnp.array([2, 6])\n",
    "    valid_len2 = mxnp.array([[2, 6],[3,4]])\n",
    "    print(f\"Shapes:query{query.shape},key{key.shape},value{value.shape},valid_len{valid_len1.shape}\")\n",
    "    r=atten(query, key, value, valid_len1)\n",
    "    print(r.shape)                #(2, 2, 4)\n",
    "    print(f\"Shapes:query{query.shape},key{key.shape},value{value.shape},valid_len{valid_len1.shape}\")\n",
    "    r2 = atten(query, key, value, valid_len2)\n",
    "    print(r2.shape)                #(2, 2, 4)\n",
    "    return r1, r2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes:query(2, 2, 2),key(2, 10, 2),value(2, 10, 4),valid_len(2,)\n",
      "(2, 2, 4)\n",
      "Shapes:query(2, 2, 2),key(2, 10, 2),value(2, 10, 4),valid_len(2,)\n",
      "(2, 2, 4)\n"
     ]
    }
   ],
   "source": [
    "r1, r2=DotProductAttention_unittest()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[[ 2.       ,  3.       ,  4.       ,  5.       ],\n",
       "         [10.       , 11.       , 12.       , 13.       ]],\n",
       " \n",
       "        [[ 4.       ,  5.       ,  6.       ,  7.0000005],\n",
       "         [ 6.       ,  7.       ,  8.       ,  9.       ]]]),\n",
       " array([[[ 2.       ,  3.       ,  4.       ,  5.       ],\n",
       "         [10.       , 11.       , 12.       , 13.       ]],\n",
       " \n",
       "        [[ 4.       ,  5.       ,  6.       ,  7.0000005],\n",
       "         [ 6.       ,  7.       ,  8.       ,  9.       ]]]))"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r1, r2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## query的shape可以自用变化了,valid_len的shape与query相关"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MLPAttention_unittest():\n",
    "    atten = MLPAttention(hidden_units=8, dropout=0.1)\n",
    "    atten.initialize()\n",
    "\n",
    "    query1=mxnp.ones((2, 2, 2))    #(2, 2,  2)\n",
    "    query2 = mxnp.ones((2, 3, 4))  # (2, 2, 4)  query的shape可以自用变化了\n",
    "    key=  mxnp.ones((2, 10, 2))  #(2, 10, 2)\n",
    "    #                       value (2, 10, 4)\n",
    "    value= mxnp.arange(40).reshape(1, 10, 4).repeat(2, axis=0)\n",
    "\n",
    "    valid_len1 = mxnp.array([2, 6])          #`valid_len`: either (`batch_size`, )\n",
    "    valid_len2 = mxnp.array([[2, 6],[3,4]])  #                 or (`batch_size`, (`batch_size`* #queries))\n",
    "\n",
    "    r1=atten(query1, key, value, valid_len2)\n",
    "    print(r1.shape)\n",
    "\n",
    "    atten = MLPAttention(hidden_units=8, dropout=0.1)\n",
    "    atten.initialize()\n",
    "    r2=atten(query2, key, value, valid_len1)\n",
    "    print(r2.shape)\n",
    "    return r1, r2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 2, 4)\n",
      "(2, 3, 4)\n"
     ]
    }
   ],
   "source": [
    "r1,r2=MLPAttention_unittest()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[[ 2.       ,  3.       ,  4.       ,  5.       ],\n",
       "         [10.       , 11.       , 12.       , 13.       ]],\n",
       " \n",
       "        [[ 4.       ,  5.       ,  6.       ,  7.0000005],\n",
       "         [ 6.       ,  7.       ,  8.       ,  9.       ]]]),\n",
       " array([[[ 2.,  3.,  4.,  5.],\n",
       "         [ 2.,  3.,  4.,  5.],\n",
       "         [ 2.,  3.,  4.,  5.]],\n",
       " \n",
       "        [[10., 11., 12., 13.],\n",
       "         [10., 11., 12., 13.],\n",
       "         [10., 11., 12., 13.]]]))"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r1,r2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "d2l",
   "language": "python",
   "name": "d2l"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
